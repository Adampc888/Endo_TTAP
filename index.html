<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision">
  <meta name="keywords" content="VL-SurgPT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://deepmind.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://deepmind-tapir.github.io/">
              TAPIR
            </a>
            <a class="navbar-item" href="https://robotap.github.io/">
              RoboTAP
            </a>
            <a class="navbar-item" href="https://deepmind-tapir.github.io/blogpost.html">
              TAPIR Blog Post
            </a>
            <a class="navbar-item" href="https://bootstap.github.io/">
              BootsTAP
            </a>
            <a class="navbar-item" href="https://tapvid3d.github.io/">
              TAPVid-3D
            </a>
            <a class="navbar-item" href="https://tap-next.github.io/">
              TAPNext
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision</h2>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                Rulin Zhou<sup>1, 3, 4</sup>,
              </span>
              <span class="author-block">
                Wenglong He<sup>1</sup>,
              </span>
              <span class="author-block">
                An Wang<sup>2, 3</sup>,
              </span>
              <span class="author-block">
                Xuanhui Zeng<sup>1</sup>,
              </span>
              <span class="author-block">
                Jianhang Zhang<sup>1</sup>,
              </span>
              <span class="author-block">
                Xi Zhang<sup>1</sup>,
              </span>
              <span class="author-block">
                Chaowei Zhu<sup>4</sup>,
              </span>
              <span class="author-block">
                Haijun Hu<sup>4</sup>,
              </span>
              <span class="author-block">
                Hongliang Ren<sup>2, 3</sup>,
              </span>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shenzhen University</span>
              <span class="author-block"><sup>2</sup>Department of Electronic Engineering, The Chinese University of Hong Kong</span>
              <span class="author-block"><sup>3</sup>The Chinese University of Hong Kong Shenzhen Research Institute</span>
              <span class="author-block"><sup>4</sup>Shenzhen People’s Hospital</span>
            </div> -->

            <!-- <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/SzuPc/VL-SurgPT-Dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data & Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/polysulfone/EVA/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Annotation Software</span>
                  </a>
                </span>
              </div> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video" style="padding-bottom: 55%">
              <video id="teaser" autoplay controls muted loop playsinline height="100%">
                <source src="https://storage.googleapis.com/dm-tapnet/tap_vid_zoom_v9_h264.mp4" type="video/mp4">
              </video>
            </div>
            <p>
              Visualization of TAP-Vid Dataset with Human Annotated Ground-Truth Point Tracks
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-3">Abstract</h3>
              <!-- 插入PDF图片，宽度100% -->
              <div style="width:100%;margin-bottom:24px;">
                <!-- 将 PDF embed 替换为图片展示 -->
                <img src="static/images/abstract.png" alt="Visualization Figure" style="width:100%;display:block;margin-bottom:24px;"/>
              </div>
              <div class="content has-text-justified">
                <p>
                  Accurate tissue point tracking in endoscopic videos is crucial for robotic-assisted surgical navigation and scene understanding, yet remains challenging due to complex tissue deformations, instrument occlusions, and the scarcity of dense trajectory annotations. Existing methods struggle with robustness in long-term tracking under these conditions due to the insufficient utilization of multimodal features and their heavy dependence on sparse annotations.
                  We present \textbf{Endo-TTAP}, addressing these challenges through a two-stage hybrid supervision approach. Our method introduces: (1) A \textbf{Multi-Facet Guided Attention} (MFGA) module that fuses multi-scale optical flow features, semantic embeddings, and motion patterns via guided attention to jointly predict point positions, occlusion states, and tracking uncertainty; (2) An \textbf{Auxiliary Curriculum Adapter} (ACA) enabling progressive domain adaptation from synthetic to surgical data through exponential coefficient scheduling; (3) A \textbf{Pseudo Label Generator} (PLG) that creates high-quality dense annotations from sparse surgical data.
                  Our two-stage training strategy first initializes components using synthetic datasets with optical flow ground truth, then transitions to real surgical data through unsupervised flow consistency and semi-supervised pseudo-label learning. We further contribute the \textbf{Endo-TTAPC5} dataset, comprising 250 video segments across five clinically meaningful challenges: tissue deformation, instrument occlusion, camera jitter, surface reflection, and cauterization smoke.
                  Extensive validation on two public datasets (SurgT, STIR) and our newly curated Endo-TTAPC5 dataset demonstrates that Endo-TTAP achieves state-of-the-art performance in tissue point tracking, particularly in scenarios characterized by complex endoscopic conditions. 
              </div>
            </div>
          </div>

          <div class="columns is-centered has-text-centered" style="margin-top: 3rem; margin-bottom: 2rem;">
            <div class="column is-full">
              <h2 class="title is-3">Robustness in Challenging Scenarios</h2>
            </div>
          </div>
                    <!-- 第一组 -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h4 class="title is-4">Tissue Deformation</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/mft_0_h264.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/ttap_0_h264.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-justified">
                Endo-TTAP substantially improves tracking accuracy, maintaining consistent performance despite frequent occlusions. This robustness is mainly due to uncertainty-aware prediction and semantic guidance, which help preserve point identities during temporary obstructions.              </p>
            </div>
          </div>

          <!-- 第二组 -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h4 class="title is-4">Instrument Occlusion</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/mft_1_h264.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/ttap_1_h264.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-justified">
                Endo-TTAP demonstrates improved stability and robustness in tracking tissue boundary points over extended sequences, especially under non-rigid motion. Even when partially occluded by surgical instruments, Endo-TTAP significantly enhances tracking accuracy and consistency. This improvement is primarily attributed to uncertainty-aware prediction and semantic guidance, which help maintain point identities during temporary occlusions.
              </p>
            </div>
          </div>

          <!-- 第三组 -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h4 class="title is-4">Camera Jitter</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/mft_2_h264.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/ttap_2_h264.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-justified">
                Endo-TTAP demonstrates a clear improvement over MFT, despite the overall error remaining elevated due to sudden endoscope movements. These abrupt shifts introduce jitter that leads to temporary misalignments, making long-term tracking more challenging.
              </p>
            </div>
          </div>

          <!-- 第四组 -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h4 class="title is-4">Surface Reflection</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/mft_3_h264.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/ttap_3_h264.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-justified">
                Endo-TTAP performs robustly under reflection conditions, showing improved tracking accuracy and demonstrating resilience to illumination artifacts. This robustness is largely attributed to the semantic embeddings from DINOv2, which provide texture-independent visual representations that help maintain tracking performance.
              </p>
            </div>
          </div>

          <!-- 第五组 -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h4 class="title is-4">Cauterization Smoke</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/mft_4_h264.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column is-half">
              <video width="512" height="384" controls autoplay muted loop>
                <source src="static/video/ttap_4_h264.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-justified">
                Endo-TTAP similarly achieves superior accuracy, indicating its effectiveness in low-visibility regions. The method’s ability to handle these challenging scenarios again benefits from the semantic guidance, ensuring reliable tracking despite visual degradation.
              </p>
            </div>
          </div>


          <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
            <div class="column is-full">
              <h4 class="title is-3">Full trajectory tracking visualization</h4>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <video width="1024" height="768" controls autoplay muted loop>
                <source src="static/video/long_video.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <p class="content has-text-centered">
                Shows the results of full trajectory annotation of a certain point on the SurgT and Endo_TAPC5 datasets.
              </p>
            </div>
          </div>



    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2211.03726">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/tapvid/tapvid.github.io">source
                code</a> of this website, which itelf is a fork of <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this
              page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
