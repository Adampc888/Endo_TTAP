<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="TAP-Vid: A Benchmark for Tracking Any Point in a Video">
  <meta name="keywords" content="TAP-Vid">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TAP-Vid: A Benchmark for Tracking Any Point in a Video</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://deepmind.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://arxiv.org/abs/2211.03726">
              TAP-Vid Dataset
            </a>
            <a class="navbar-item" href="https://deepmind-tapir.github.io/">
              TAPIR
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TAP-Vid: A Benchmark for Tracking Any Point in a Video</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Jvi_XPAAAAAJ">Carl Doersch</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.robots.ox.ac.uk/~ankush/">Ankush Gupta</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.ru/citations?user=jM6Y0yAAAAAJ">Larisa Markeeva</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://people.csail.mit.edu/recasens/">Adrià Recasens</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=yN7CEicAAAAJ">Lucas Smaira</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://people.csail.mit.edu/yusuf/">Yusuf Aytar</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ">João Carreira</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a><sup>1,2</sup>
              </span>
              <span class="author-block">
                <a href="https://yangyi02.github.io">Yi Yang</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google DeepMind,</span>
              <span class="author-block"><sup>2</sup>VGG, Department of Engineering Science, University of Oxford</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/pdf?id=Zmosb2KfzYd"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2211.03726" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/deepmind/tapnet"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data & Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="publication-video" style="padding-bottom: 55%">
          <video id="teaser" autoplay controls muted loop playsinline height="100%">
            <source src="https://storage.googleapis.com/dm-tapnet/tap_vid_zoom_v9.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Generic motion understanding from video involves not only tracking objects, but also perceiving how their
              surfaces deform and move. This information is useful to make inferences about 3D shape, physical
              properties and object interactions. While the problem of tracking arbitrary physical points on surfaces
              over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until
              now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a
              companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations
              of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction
              of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to
              compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder
              sections of video. We validate our pipeline on synthetic data and propose a simple end-to-end point
              tracking model TAP-Net, showing that it outperforms all prior methods on our benchmark when trained on
              synthetic data.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <h2 class="title is-3">Video Summary</h2>
            Please see <a href="https://nips.cc/virtual/2022/poster/55696">Neurips Presentation</a> for the summary of
            the work.
            <!-- <video id="summary" playsinline controls muted height="100%">
              <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/summary_video_v5.mp4"
                type="video/mp4">
            </video> -->
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <div class="is-centered" style="border-bottom: 1px solid darkgray; border-top: 1px solid darkgray;">
        <p>Here we show the example tasks that we tackled with RoboTAP. For each task, the system only saw 4-6
          demonstrations of the behavior; outside of these demonstrations, the relevant objects have never been seen
          before. Click the icons on the bottom to see different examples of the robot at work.</p>
        <div style="margin:auto">
          <table style="width:1024;padding-bottom:0px" cellpadding="0" cellspacing="0">
            <tr>
              <td style="padding-left:4px;padding-right:4px;">
                <div style="display:inline;">
                  <video id="success_vid0" width="352" height="198" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div>
              </td>
              <td style="padding-left:4px;padding-right:4px;">
                <div style="display:inline;">
                  <video id="success_vid1" width="352" height="198" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div>
              </td>
              <td style="padding-left:4px;padding-right:4px;">
                <div style="display:inline;">
                  <video id="success_vid2" width="352" height="198" autoplay loop muted>
                    <source src="" type="video/mp4" />
                  </video>
                </div>
              </td>
            </tr>
          </table>
        </div>

        <p id="text">Javascript required.</p>
        <br />
        <h3 style="border-bottom: 1px solid darkgray;">Gallery</h3>
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/apple_on_jello.png" width="78"
          onclick="set_source(0);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/four_block_stack.png" width="78"
          onclick="set_source(1);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/four_block_stencil_w2.png"
          width="78" onclick="set_source(2);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/gluing.png" width="78"
          onclick="set_source(3);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/juggling_stack.png" width="78"
          onclick="set_source(4);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/lego_stack_w3.png" width="78"
          onclick="set_source(5);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/pass_butter.png" width="78"
          onclick="set_source(6);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/precision.png" width="78"
          onclick="set_source(7);" />
        <img id="input" style="margin-top:10px;cursor:pointer;"
          src="https://storage.googleapis.com/dm-tapnet/robotap/videos/success_gallery/tapir_robot_v2.png" width="78"
          onclick="set_source(8);" />
      </div>

    </div>

    <div class="container is-max-desktop">

      <div class="is-centered has-text-centered">
        <div class="is-four-fifths">
          <div class="content">
            <h2 class="title is-3">The RoboTAP Approach</h2>
            <div class="content has-text-justified">
              <p>
                What makes RoboTAP different from other robotics algorithms is that it uses <a
                  href="https://github.com/deepmind/tapnet">TAP</a>, and specifically the state-of-the-art <a
                  href="https://deepmind-tapir.github.io/">TAPIR</a> model, as the core of its spatial understanding. We
                show that this model can provide sufficient pose estimation, and even segmentation, for novel objects.
                <br /><br />
                RoboTAP begins with a set of demonstrations. For the stencil task, a demonstration looks like this. Note
                that the user is positioning the robot by pushing it (so-called &ldquo;kinesthetic teaching&rdquo;).
                This contrasts with many imitation learning algorithms like behavioral cloning, which require access to
                the control signals that produce the action.
              </p>
            </div>
            <video id="teaser" autoplay controls muted loop playsinline height="100%">
              <source
                src="https://storage.googleapis.com/dm-tapnet/robotap/videos/four_block_stencil_w2_demonstration.mp4"
                type="video/mp4">
            </video>
            <div class="content has-text-justified">
              <p>
                Given these demonstrations, RoboTAP automatically breaks the action down into stages, and extract a set
                of <i>active points</i> that determine the motion for each stage.
                For the stencil task, we have a total of 8 stages: four stages where points on the successive blocks are
                &ldquo;active&rdquo; (i.e., the robot reaches toward the object), and another four stages where the
                points on the stencil are &ldquo;active&rdquo; (i.e., stages where the robot must place each object at a
                specific location relative to the stencil).
                <br /><br />
                The stages come from gripper actions (we assume that closing or opening the gripper marks the start of a
                new stage).
                Then we track a few thousand randomly-selected points using TAPIR, across all demos, and then select the
                points which are relevant for each stage.
                Specifically, within each stage, RoboTAP selects TAPIR points which all arrive at the same location at
                the end of the segment, suggesting that the user's goal is to place the object at a specific location.
                Here's the &ldquo;active points&rdquo; for the stencil task. Each point is tracked automatically by
                TAPIR.
              </p>
            </div>

            <video id="teaser" autoplay controls muted loop playsinline height="100%">
              <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/robot_active_points.mp4"
                type="video/mp4">
            </video>
            <div class="content has-text-justified">
              <p>
                In order to produce clean active point sets that track the relevant object even under noise and
                occlusion, we find it's helpful to perform motion-based unsupervised object discovery. That is, we break
                the scene down into a set of &ldquo;objects&rdquo; that can move independently and explain the point
                motion as well as possible. This problem has classically proved challenging in the computer vision
                literature, but with strong point tracking from TAPIR (and, in particular, its very low rate of false
                positives), it becomes feasible. Here's a set of objects automatically discovered from the stencil
                demonstrations:

              </p>
            </div>

            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/clustering_v2.mp4" type="video/mp4">
            </video>

            <div class="content has-text-justified">
              <p>Given a set of active points for each stage of the demo, we next construct a &ldquo;motion plan&rdquo;
                which defines the full behavior. Note that the motion plan simply consists of a set of &rdquo;active
                points,&ldquo; the paths they followed during the demonstrations, as well as gripper actions at the end
                of each stage. Therefore, the inferred behavior can be easily understood and potentially even edited if
                desired, although we made no edits for the behaviors shown in this paper.
                <br />
                <!--<img src="./static/images/salient_point_selection_v3.svg" class="interpolation-image"
              alt="RoboTAP summary." /><br/>-->
                <img src="./static/images/overview_figure_v2.svg" class="interpolation-image"
                  alt="TAPIR architecture." /><br />
                Given the motion plan, RoboTAP can run on the robot by imitating each stage using standard visual
                servoing.
                That is, we compute an approximate Jacobian of the motion of the points with respect to gripper motions,
                and execute actions which bring the points toward the trajectories seen in the demos. Here's an example
                of the full behavior executed on the robot (5x speed):

              </p>
            </div>

            <div class="publication-video" style="padding-bottom: 30%">
              <video id="teaser" autoplay controls muted loop playsinline height="100%">
                <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/all_vis.mp4" type="video/mp4">
              </video>
            </div>
            <div class="content has-text-justified">
              <p>
                <!--               RoboTAP using points <b>automatically selected</b> from few demos (≤6) to define motion. There are three videos shown above.-->
                The left video shows the third-person view. The middle video shows motion at inference time overlayed
                over the closest demo for each stage.
                The &ldquo;active points&rdquo; relevant to each stage are shown in <font color="red">red</font>, and
                goal locations
                extracted from the demos are shown <font color="cyan">cyan</font>. The third video illustrates the
                desired motion: the visual servoing produces a 4D (x, y, z, rz) robot action which moves each point in
                the direction indicated by the <font color="blue">blue lines</font>, therefore moving toward the
                trajectory seen in the demos.
                <!-- The videos below show the identified key points from the six demonstrations used to solve the task. -->
                <br /><br />
                RoboTAP can already solve a wide variety of tasks, including many that are analogous to the precise and
                repetitive tasks common in industrial settings. However, there are also limitations. For one, the system
                is purely visual and does not incorporate force feedback. Furthermore, it is not designed to dynamically
                compose skills if the task needs to be done in a different order than what was seen in the
                demonstrations. TODO: say more about failure cases.
              </p>
            </div>
            <div class="content">

              <div class="content has-text-justified">
                <p>
                  Here we explore some of the limitations of this approach.
                </p>
              </div>
              <div class="container">
                <div id="failure-carousel" class="carousel results-carousel">
                  <div class="item">
                    <video poster="" id="4_object_stack_fail" autoplay controls muted loop playsinline height="100%">
                      <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/4_object_stack_12_fail.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="4_object_stack_fail" autoplay controls muted loop playsinline height="100%">
                      <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/4_object_stack_19_fail.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="apple_jello_sticky_fail" autoplay controls muted loop playsinline
                      height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/apple_jello_sticky_20_fail.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="stencil_fail" autoplay controls muted loop playsinline height="100%">
                      <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/gluing_19_partial.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="stencil_fail" autoplay controls muted loop playsinline height="100%">
                      <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/lego_stack_w3_8.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="stencil_fail" autoplay controls muted loop playsinline height="100%">
                      <source src="https://storage.googleapis.com/dm-tapnet/robotap/videos/four_block_stencil_w2_10.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
              <p id="failure-text">Javascript Required</p>
            </div>
          </div>

        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <h2 class="title is-3">RoboTAP Dataset</h2>
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                  <div class="item">
                    <video poster="" id="4_object_stack" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/10790317943435558913.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="apple_on_jello" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/gear_v2_9-basket_front_left_rgb_img-2.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id="four_block_stencil" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/nist_board1-robot0_camera0_rgb_img.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=gluing"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/nist_cables1-robot0_camera0_rgb_img.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=juggling_stack"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/nist_nuts_bolts1-robot0_camera0_rgb_img.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=lego_stack"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/plush1-robot0_camera0_rgb_img.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=pass_butter"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/sketchy1-pixels_basket_front_left.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=tapir_robot"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/sketchy4-pixels_basket_front_left.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                  <div class="item">
                    <video poster="" id=tapir_robot"" autoplay controls muted loop playsinline height="100%">
                      <source
                        src="https://storage.googleapis.com/dm-tapnet/robotap/videos/dataset/sketchy4-pixels_usbcam1.mp4"
                        type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
              <div class="content has-text-justified">
                <p>
                  The ability to track and relate points in the scene is what enabled RoboTAP to generalize to novel
                  scenes and poses. These capabilities are directly powered by the performance of these models and
                  therefore a core part of our contribution is enabling advances in these areas. To enable better
                  research
                  we introduce an addition to the previous TAP-Vid benchmark in a form of a new dataset which focuses
                  specifically on robotic manipulation.
                </p>
              </div>
            </div>
          </div>
        </div>

        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Related Links</h2>

            <div class="content has-text-justified">
              <p>
                <a href="https://particle-video-revisited.github.io/">Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories</a> propose Persistent Independent Particles (PIPs), a new particle video method that tracks any pixel over time.
              </p>
            </div>
          </div>
        </div>

      </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2211.03726">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/tapvid/tapvid.github.io">source
                code</a> of this website, which itelf is a fork of <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this
              page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>